{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the next hit song\n",
    "An exploratory analysis on maximising my chances of making the next hit song.  \n",
    "Explore the Spotify dataset for features that may give me the edge on writing a popular song.  \n",
    "  \n",
    "Dataset: https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup folder heirarchy and load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_DATA_RAW = os.path.join(\".\", \"data\", \"external\")\n",
    "\n",
    "for dir, dirs, files in os.walk(DIR_DATA_RAW):\n",
    "    for file in files:\n",
    "        print(os.path.join(DIR_DATA_RAW, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'.\\\\data\\\\external\\\\tracks.csv' does not exist: b'.\\\\data\\\\external\\\\tracks.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2b90ae7feea1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDIR_DATA_RAW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tracks.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'release_date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'.\\\\data\\\\external\\\\tracks.csv' does not exist: b'.\\\\data\\\\external\\\\tracks.csv'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(os.path.join(DIR_DATA_RAW, 'tracks.csv'), index_col=0, parse_dates=['release_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feel for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS_OF_DATA = 100\n",
    "df = df[df.release_date > df.release_date.max() - pd.DateOffset(years=YEARS_OF_DATA)].sort_values(by='release_date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()  # 587k rows, all numerical except name, artists, id_artists and release_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()  # look at NaN entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if there's anything interesting about NaN rows that might indicate larger problems with dataset\n",
    "df.loc[df.name.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no larger issues identified - drop rows\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# drop rows with time_signature as 0\n",
    "df = df[df.time_signature != 0]\n",
    "df.describe()['time_signature']['min']\n",
    "\n",
    "# duration in seconds\n",
    "df['duration_ms'] = df.duration_ms/1.0e3\n",
    "df.rename(columns = {'duration_ms': 'duration_s'}, inplace=True)\n",
    "dtypes = df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate categorical and numerical entries as they are analysed differently\n",
    "y = ['popularity']  # column of interest\n",
    "cols_numerical = ['duration_s', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "cols_categorical = ['explicit', 'release_date', 'key', 'mode', 'time_signature']\n",
    "\n",
    "\n",
    "# plot distributions of each numerical feature\n",
    "cols = 3  # 3 column grid\n",
    "rows = int(np.ceil(1.0*len(cols_numerical)/3))\n",
    "figure, axes = plt.subplots(rows, cols, figsize=(14,8)) # create grid (may be a few spare plots in the final row)\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "for col in cols_numerical:\n",
    "    sns.distplot(df[col], ax=axes[i, j]) \n",
    "    j += 1 # next column\n",
    "    if(j >= cols): # if we would next plot at column 4, move to first column in the next row\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "figure.tight_layout() # fix spacing issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 tracks in key of C minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['key'] == 0) & (df['mode'] == 0)].sort_values('popularity', ascending=False)[['name', 'artists', 'popularity', 'key', 'mode']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 3 most popular solo artists\n",
    "### does not account for solo + collaborations; accounts for solo releases only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.copy()\n",
    "a = df.groupby('artists').mean().sort_values(by='popularity', ascending=False)  # group artists and sort by mean popularity\n",
    "a['num_artists'] = [len(x.split(', ')) for x in a.index]  # create column that counts number of artists for each song\n",
    "a[a['num_artists'] == 1].head(3)  # display top 3 solo artists (already sorted in descending order from two lines ago)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What makes a song popular?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df[cols_numerical + y].corr() # correlation coefficients don't make sense for categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRELATION_THRESH = 0.2  # arbitrary correlation threshold (0 to 1) to highlight most correlated entries on right-hand side\n",
    "highly_correlated_indices = np.multiply(df_corr.abs() > CORRELATION_THRESH, df_corr.abs() != 1)  \n",
    "# .abs() to account for negatively-correlated entries\n",
    "# !=1 to remove the main diagonal (entries are, of course, perfectly correlated with themselves)\n",
    "\n",
    "np.multiply(highly_correlated_indices, df_corr).style.background_gradient(cmap='viridis')\n",
    "\n",
    "f, a = plt.subplots(1,2, figsize=(20,8));\n",
    "plt.subplot(1,2,1)\n",
    "sns.heatmap(df_corr);\n",
    "a[0].set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arbitrary correlation threshold (0 to 1) to highlight most correlated entries on right-hand side\n",
    "CORRELATION_THRESH = 0.15\n",
    "highly_correlated_indices = np.multiply(df_corr.abs() > CORRELATION_THRESH, df_corr.abs() != 1)  \n",
    "# .abs() to account for negatively-correlated entries\n",
    "# !=1 to remove the main diagonal (entries are, of course, perfectly correlated with themselves)\n",
    "\n",
    "f, a = plt.subplots(1,2, figsize=(20,8));\n",
    "f.suptitle('Correlation coefficients of numerical data', fontsize=26)\n",
    "\n",
    "plt.subplot(1,2,1).set_title('Correlation coefficients', fontsize=16)\n",
    "sns.heatmap(df_corr);\n",
    "\n",
    "plt.subplot(1,2,2).set_title('Correlation coefficients above threshold', fontsize=16)\n",
    "sns.heatmap(np.multiply(highly_correlated_indices, df_corr));\n",
    "\n",
    "# energy and loudness are highly correlated\n",
    "# drop loudness or energy to avoid highly correlated columns disproportionately effecting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at popularity column and remove entry w.r.t. itself\n",
    "df_corr['popularity'].drop('popularity').sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy is highly correlated with loudness, but is less correlated with popularity than loudness, so drop energy\n",
    "cols_numerical.remove('energy')\n",
    "df.drop(['energy'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore attributes that contain discrete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 1\n",
    "cols = 3\n",
    "plt.subplots(rows,cols, figsize=(13,4))\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "# musical key only makes sense plotted with the mode (major or minor)\n",
    "plt.subplot(rows,cols,1)\n",
    "plt.title('Musical key vs mean popularity')\n",
    "sns.barplot(x='key', y='popularity', hue='mode', data=df)\n",
    "\n",
    "plt.subplot(rows,cols,2)\n",
    "plt.title('Time signature vs mean popularity')\n",
    "sns.barplot(x='time_signature', y='popularity', data=df);\n",
    "\n",
    "plt.subplot(rows,cols,3)\n",
    "plt.title('Explicit vs mean popularity')\n",
    "sns.barplot(x='explicit', y='popularity', data=df);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings so far\n",
    "- Most popular keys are 1, 6 and 7, with mode 0, which corresponds to C# minor, F# minor and G# minor\n",
    "- 4/4 time signature is most popular on average\n",
    "- Explicit songs are more popular than non-explicit songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = [x.year for x in df.release_date]  # add column for year of release\n",
    "df[['year','release_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_by_year = df.groupby('year').mean()  # group by year\n",
    "tracks_by_year = tracks_by_year.sort_values('year')\n",
    "# doesn't make sense to use mean value of categorical fields, so drop categorical columns\n",
    "tracks_by_year.drop(['explicit', 'key', 'mode', 'time_signature'], axis=1, inplace=True)\n",
    "tracks_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out low correlation noise\n",
    "tracks_by_year_normalised = tracks_by_year.copy()\n",
    "CORRELATION_THRESH = 0.15 \n",
    "\n",
    "# normalise variables to improve readibility from graph\n",
    "for col in tracks_by_year.columns:  \n",
    "    mean = tracks_by_year[col].mean()\n",
    "    std_dev = tracks_by_year[col].std()\n",
    "    tracks_by_year_normalised[col] = (tracks_by_year[col] - mean)/std_dev\n",
    "    \n",
    "# create plot\n",
    "figure, axes = plt.subplots(figsize=(15,6));\n",
    "l = []  # initiate list for legend\n",
    "for col in tracks_by_year_normalised.columns:  # plot if correlated enough with popularity\n",
    "    if(df_corr['popularity'].abs()[col] > CORRELATION_THRESH):\n",
    "        sns.lineplot(x=tracks_by_year_normalised.index, y=col, data=tracks_by_year_normalised)\n",
    "        l.append(col)\n",
    "\n",
    "#  looks like at least 4 local minima and maxima on popularity curve\n",
    "#  choose 5th degree polynomial or greater\n",
    "d = 6  # polynomial degree\n",
    "curve = np.poly1d(np.polyfit(tracks_by_year_normalised.index, tracks_by_year_normalised.popularity, d));  # get poly coefficients\n",
    "x = range(tracks_by_year.index.min(), tracks_by_year.index.max());  # create array of integers from start year to end year\n",
    "plt.plot(x, curve(x), color='black')  # overlay best fit curve with calculated (x, curve(x))\n",
    "\n",
    "l.append('popularity line of best fit')\n",
    "plt.legend(labels=l);\n",
    "axes.set_ylabel('magnitude', fontsize=15);\n",
    "axes.set_xlabel('year', fontsize=15);\n",
    "axes.set_title('variables more highly correlated with popularity (normalised)', fontsize=20);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## To maximise chances at writing a pop hit, write a song:\n",
    "- in one of the following keys: C#, F# or G# minor\n",
    "- in 4/4 time\n",
    "- that you can dance to (turn up the volume)\n",
    "- include some explicit language\n",
    "- must not be purely instrumental\n",
    "- must not be accoustic\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
